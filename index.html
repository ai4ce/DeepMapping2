<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Self supervised VPR method to detect spatial Neighborhoods">
  <meta name="keywords" content="VPR, Self-Supervised, SLAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DeepMapping2</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="shortcut icon" type="image/x-icon" href="./static/images/ai4ce.png" />
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://ai4ce.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <!-- <img src="./static/images/logo.png" width="45"> -->
            DeepMapping2&nbsp;&nbsp
          </h1>
          <h1 class="title is-2 publication-title">Self-Supervised Large-Scale LiDAR Map Optimization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://joechencc.github.io">Chao Chen*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://gaaaavin.github.io">Xinhao Liu*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=i_aajNoAAAAJ">Yiming Li</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://www.hajim.rochester.edu/ece/lding6/">Li Ding</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>New York University, Brooklyn, NY 11201, USA</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup>University of Rochester, Rochester, NY 14627, USA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                 <!-- add here later. -->
                <a href="https://arxiv.org/abs/2212.06331"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <!-- add here later. -->
               <!-- <a href=""                    
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fas fa-camera"></i>
                 </span>
                 <span>Appendix</span>
               </a> -->
             </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ai4ce/DeepMapping2"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1jWfpmCAXaO9122mdkAuSV-yq_2DanPRh"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" controls loop height="100%">
        <source src="./static/video/Multimedia.mp4"
                type="video/mp4">
      </video> -->
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <!-- <br><br><br> -->
      <!-- <h2 class="subtitle has-text-centered">
      Project explanation
      </h2> -->
    <!-- <h2 class="subtitle has-text-centered">
      (The <span style="color:#000000;">black</span> / <span style="color:#ff0000;">red</span> lines are the ground truth / predicted camera trajectory)
    </h2> -->    
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="./static/images/SUBT_video.mp4"
                type="video/mp4">
      </video> -->
      <img src="./static/images/kitti_map.jpg" height="300">
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <br><br><br>
      <h2 class="subtitle has-text-centered">
          <b>Mapping result on the KITTI dataset.</b> The bird's eye view map is shown together with the estimated sensor pose of each frame. Each pose is represented by an arrow indicating the xy-coordinate and heading (yaw angle) as shown in the bottom examples. The color indicates the frame index in the trajectory. Challenging regions are zoomed in for a clear view. Best viewed in color.
    </h2>
    <!-- <h2 class="subtitle has-text-centered">
      (The <span style="color:#000000;">black</span> / <span style="color:#ff0000;">red</span> lines are the ground truth / predicted camera trajectory)
    </h2> -->
    <!-- <h2 class="is-size-6 has-text-centered">(The left are few-shot annotations online provided by user and right is the detection results)</h2> -->
    
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/V5hYTz5os0M?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
         <p>
          LiDAR mapping is important yet challenging in self-driving and mobile robotics.
          To tackle such a global point cloud registration problem, DeepMapping converts the complex map estimation into a self-supervised training of simple deep networks.
          </p>
          <p>
          Despite its broad convergence range on small datasets, DeepMapping still cannot produce satisfactory results on large-scale datasets with thousands of frames. This is due to the lack of loop closures and exact cross-frame point correspondences, and the slow convergence of its global localization network.
          </p>
          <p>
          We propose DeepMapping2 by adding two novel techniques to address these issues:
          <li> organization of training batch based on map topology from loop closing
          </li>
          <li> self-supervised local-to-global point consistency loss leveraging pairwise registration.
          </li>
          <li> Our experiments and ablation studies on public datasets (KITTI, NCLT, and Nebula) demonstrate the effectiveness of our method. Our code will be released.
          </li>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <hr>
        <h2 class="title is-3">Method</h2>
        <br>
        <img src="./static/images/pipeline2.png" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
          Our contributions are summarized as follows:
            <li> 
                Our DeepMapping2 is the first self-supervised large-scale LiDAR map optimization method as far as we know, and this generic method achieves state-of-the-art mapping results on various indoor/outdoor public datasets, including KITTI, NCLT, and the challenging underground dataset Nebula. Our code will be released to ensure reproducibility.
            </li>
            <li> 
                Our analysis reveals why DeepMapping fails to scale up and leads to the two novel techniques to incorporate loop closing and local registration in the DeepMapping framework. The necessity and effectiveness of the two techniques are further validated in our ablation study.
            </li>
          </p>
        </div>
      </div>
    </div>
    <hr>

    
    <!-- Applications.-->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <h2 class="title is-3">Dataset and Mapping results</h2>
      </div>
    </div>
    <p>
      &nbsp
    </p>
    <div class="column is-full_width">
      <img src="./static/images/kitti_map.jpg" width=1080 border=2px class="center"/> &nbsp;&nbsp;
      <img src="./static/images/nclt_map.jpg"  width=1080 border=2px class="center"/> &nbsp;&nbsp;
      <img src="./static/images/nebula_map.jpg" width=1080 border=2px class="center"/>
      <div class="content has-text-justified">
        <br>  
        <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) <a href="https://www.cvlibs.net/datasets/kitti/">KITTI Data</a>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) <a href="http://robots.engin.umich.edu/nclt/">NCLT Data</a>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) <a href="https://github.com/NeBula-Autonomy/nebula-odometry-dataset">Nebula Data</a>
          <br>
          <br>
          We employ three datasets for a comprehensive evaluation: (1) KITTI dataset for evaluations in outdoor scenarios, (2) NeBula odometry dataset for evaluations in indoor environments where GPS signal is not available, and (3) campus-level NCLT dataset for evaluations in both indoor and outdoor scenarios.
        </p>

        <p>
          <em><b>KITTI dataset</b></em> is a widely-used authoritative benchmark to evaluate SLAM-based algorithms. We employ the two most complex and challenging scenes from the dataset, where lots of places are revisited multiple times and the explored area is relatively large. Meanwhile, there are dynamic objects on the road, further increasing the difficulties.
        </p>
        <p>
          <em><b>NCLT dataset</b></em> is a large-scale LiDAR dataset collected in the University of Michigan’s North Campus. The point clouds are collected using a Velodyne HDL-32E 3D LiDAR mounted on a Segway robot. Overall, the NCLT dataset has a robot trajectory with a length of 147.4 km and maintains 27 discrete mapping sessions over the year. Each mapping session includes both indoor and outdoor environments. We select an interval of the trajectory for better illustration.
        </p>
        <p>
          <em><b>Nebula dataset:</b></em> is provided by the Team CoSTAR from the Jet Propulsion Laboratory at Caltech. The dataset has been used to test the multi-robot system in real-world environments such as tunnels and caves for the DARPA Subterranean Challenge. In each scene of the dataset, a ground-truth survey-grade map is provided by DARPA and the trajectory is produced by running LOCUS 2.0 on the map. Besides the point clouds, visual odometry and kinematic odometry are also included. We employ the data collected in the cave in Lava Beds National Monument. The whole trajectory is 590.85 meters long and contains 37,949 LiDAR scans in total.
        </p>
      </div>
    </div>
    <p>
      &nbsp
    </p>
    <p>
      &nbsp
    </p>


    <section class="section">
      <div class="container is-max-desktop">
        <!-- <br> -->
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Training animation</h2>
            <div class="column is-full_width">
              <img src="static\images\KITTI_0018.gif" width=1000 height=90% overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration one <em><b>KITTI</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. Each pose is represented by an arrow indicating the xy-coordinate and heading (yaw angle) as shown in the bottom examples. The color indicates the frame index in the trajectory.
              </h3>
              <p>
                &nbsp
              </p>
              <img src="static\images\KITTI_0027.gif" width=1000 height=100px overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration another <em><b>KITTI</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. The representation is the same as above.
              </h3>
              <p>
                &nbsp
              </p>
              <img src="static\images\NCLT_0108.gif" width=1000 height=100px overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration one <em><b>NCLT</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. The representation is the same as above.
              </h3>
              <p>
                &nbsp
              </p>
              <img src="static\images\Nebula_E.gif" width=1000 height=100px overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration another <em><b>Nebula</b></em> mapping result with respect to the epochs. The ground truth in this dataset is not accurate, thus translation ATE and rotation ATE are not provided. The representation is the same as above.
              </h3>
              <div class="content has-text-justified">
         
              </div>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>
    </section>


    


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{chen2022deepmapping2,
        title={DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization},
        author={Chen, Chao and Liu, Xinhao and Li, Yiming and Ding, Li and Feng, Chen},
        journal={arXiv preprint arXiv:2212.06331},
        year={2022}
      }
    </code></pre>
  </div>
</section>

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    Chen Feng is the corresponding author. The research is supported by NSF Future Manufacturing program under CMMI-1932187, CNS-2121391, and EEC-2036870. Chao Chen gratefully thanks the help from Pratyaksh P. Rao and Pareese Pathak.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
